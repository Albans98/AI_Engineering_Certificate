{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<h2 align=center>Helper functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7eff8404c270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2 align=center>Prepare Data </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7eff08db8828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "### Build a Convolutional Neral Network Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAADrCAYAAABNVDkBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADu0lEQVR4nO3dPW4TURRA4Xv5kQJBIkVCQeMy7jMtYhnswItgKd4Fm6Ayfdwgyki4cJGC7lLQBMkwGunNT3LO144l39GR3yjSe5msqtDT9mzuATQ+IwMYGcDIAEYGMDLAi74PZOYmIjYREednZzfr1Wr0oTTcj7u7OByPeepaDvk7uVuva7fdNhtM7XSbTexub09GdrkGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAL2RM3OTmbvM3P08HqeYSY31Rq6qbVV1VdVdXVxMMZMac7kGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGaD39QQPfdvfR378OtYsM/g09wANvfznlUHvoIhwc/1jNOgERcT5FDOpMZ/JAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcATFACeoABwuQYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZIKvq/x/4a3N9XEfEfuyhIuIyIg4TfM8UprqXVVVdnbrQG3kOmbn7s5n/8VvCvbhcAxgZYKmRt3MP0NDs97LIZ7LaWuovWQ0ZGcDIAEYGMDLAwP8Z8vwm4s3II01p0JuFF+4+qn7lqSuD/oTKvKiID83Gmt+7uQdo6EtUHU5GdrkGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAANPULwaeRyNwRMUT4YnKNCMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGGLTv+n1mbfo/9mis5x6goc8R8b3q5L7rQSco3radSxPpXa6raltVXVV1r6eYSM35TAYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwM31AG6uB3C5BjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyAC976B4eIIiIq4jYj/2UBFxGRGHCb5nClPdy6qqrk5dGPSikalk5q6qurnnaGEJ9+JyDWBkgKVG3s49QEOz38sin8lqa6m/ZDVkZAAjAxgZwMgAvwF376R/TskBAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAACqCAYAAACTZZUqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEFUlEQVR4nO3asYpdVRiG4X/JFDISBsLYBNMkiBamMecybLyJfQfeh6VMmTvILZj2dE5pKwQiQ2wC2vwWWpzAxJ1tztlfzpzn6QZ2WB/D4iUsZnR3AbC+T9IDAE6VAAOECDBAiAADhAgwQIgAA4SczX0wxpiqavrnp0+fVn1x4El3yav0gCPzprr/GmuctHuvPzs/f/r1o0drHHsn/Hl9nZ5wVH6rqpvuW+/1WPJ3wGN82VU/7mvXCfgpPeDIvKju16sEeNfmyZPePn++9rFH69fHj9MTjsr3VfXLOwLsCQIgRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCZgM8xpjGGNsxxrbqjzU2wcHt3utXNzfpOZyo2QB391V3b7p7U3WxxiY4uN17/fn9++k5nChPEAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACGju9/74wdj9HTAMXfNmx/e/3dL1bNnm3r5cjvWPvfbMfrntQ89Yvfqu/SEI/Oiul/feq/P5v7pGGOqqqmq6mLPsyBl914/DG/hdM0+QXT3VXdvuntzvsYiWMHuvb5Mj+FkeQMGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAkLO5D8YYU1VNVVUXB58D69i91w/DWzhds/8D7u6r7t509+Z8jUWwgt17fZkew8nyBAEQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAho7v/+4Mxpqqa/v3xm6q6PvSo/+Gyqn5Pj7iFXct81d331jjIvf4gdi3zzns9G+C3Ph5j292bvc3aE7uWsevjOHeOXcsc4y5PEAAhAgwQsjTAVwdZ8eHsWsauj+PcOXYtc3S7Fr0BA7A/niAAQgQYIESAAUIEGCBEgAFC/gYhD8L73BBEIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " optimizer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "<h2 align=center>Analyse Results</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
